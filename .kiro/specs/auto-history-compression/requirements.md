# 自动历史消息压缩功能需求文档

## 1. 功能概述

为智能体添加自动历史消息压缩能力。当智能体的历史消息超过设定的 tokens 阈值时，系统自动通过独立的大模型请求压缩较早的历史消息。这个过程对智能体完全透明，智能体无需主动调用 `compress_context` 工具。

## 2. 需求背景

### 2.1 现状分析

当前系统已经具备以下能力：
- ConversationManager 管理智能体的对话历史和 token 使用统计
- 提供 `compress_context` 工具供智能体主动压缩历史
- 在消息中注入上下文状态提示，提醒智能体压缩历史
- 当上下文超过硬性限制时拒绝 LLM 调用

### 2.2 存在的问题

- 依赖智能体主动调用 `compress_context` 工具
- 智能体可能忽略上下文警告，导致超过硬性限制
- 智能体需要自己编写压缩摘要，增加了智能体的负担
- 压缩时机不够及时，可能在接近限制时才压缩

### 2.3 解决方案

实现自动历史消息压缩机制：
- 在 LLM 调用前自动检查上下文使用率
- 达到阈值时自动触发压缩流程
- 通过独立的 LLM 请求生成压缩摘要
- 压缩过程对智能体透明，无需智能体参与

## 3. 功能需求

### 3.1 自动压缩触发机制

**需求 ID**: REQ-3.1

**描述**: 在 LLM 调用前自动检查上下文使用率，达到阈值时触发自动压缩。

**验收标准**:
- 在 `LlmHandler.handleWithLlm()` 中，调用 LLM 前检查上下文使用率
- 当使用率达到自动压缩阈值（默认 80%）时，触发自动压缩流程
- 自动压缩阈值可配置，通过 ConversationManager 的配置项设置
- 自动压缩流程完成后，继续正常的 LLM 调用流程
- 如果自动压缩失败，记录错误日志但不阻止 LLM 调用

### 3.2 自动生成压缩摘要

**需求 ID**: REQ-3.2

**描述**: 通过独立的 LLM 请求自动生成历史消息的压缩摘要。

**验收标准**:
- 创建专门的摘要生成提示词模板
- 提取需要压缩的历史消息（保留最近 N 条消息）
- 调用独立的 LLM 请求生成摘要
- 摘要生成使用的模型由用户在配置文件中配置
- 摘要应包含关键信息：任务目标、重要决策、已完成的工作、待办事项
- 摘要长度控制在合理范围内（建议 500-1000 tokens）
- 摘要生成失败时，忽略本次压缩，不执行压缩操作

### 3.3 执行自动压缩

**需求 ID**: REQ-3.3

**描述**: 使用生成的摘要执行历史消息压缩，同时保留完整的历史记录用于持久化。

**验收标准**:
- 在运行时内存中执行压缩，保留系统提示词、压缩摘要和最近的 N 条消息（默认 10 条）
- 压缩后清除 token 使用统计，等待下次 LLM 调用时重新统计
- 记录压缩前后的消息数量和上下文使用率
- **持久化时保留完整的历史消息**，不受运行时压缩影响
- 在持久化数据中记录压缩信息：
  - 压缩发生的位置（哪些消息被压缩）
  - 压缩的摘要内容
  - 压缩时间戳
  - 被压缩的原始消息（完整保留）
- 持久化数据支持压缩内容的可展开查看

### 3.4 硬性限制截断

**需求 ID**: REQ-3.4

**描述**: 当摘要生成失败且 tokens 超过硬性限制时，截断消息历史以确保不超限。

**验收标准**:
- 在检查硬性限制时，如果超限则触发截断流程
- 保留系统提示词（第一条消息）
- 保留最近的 N 条消息（默认 10 条）
- 删除中间的消息
- 清除 token 使用统计
- 记录截断前后的消息数量
- 持久化截断后的对话历史

### 3.5 配置项管理

**需求 ID**: REQ-3.5

**描述**: 提供自动压缩功能的配置项。

**验收标准**:
- 在 ConversationManager 中添加自动压缩配置项：
  - `autoCompressionEnabled`: 是否启用自动压缩（默认 true）
  - `autoCompressionThreshold`: 自动压缩阈值（默认 0.8，即 80%）
  - `autoCompressionKeepRecentCount`: 压缩时保留的最近消息数量（默认 10）
  - `autoCompressionSummaryMaxTokens`: 摘要最大 token 数（默认 1000）
  - `autoCompressionSummaryModel`: 摘要生成使用的模型（由用户在配置文件中配置，无默认值）
- 配置项可通过构造函数参数传入
- 配置项可在运行时动态修改

### 3.6 日志和监控

**需求 ID**: REQ-3.6

**描述**: 记录自动压缩过程的详细日志，便于监控和调试。

**验收标准**:
- 记录自动压缩触发时的上下文状态
- 记录摘要生成的 LLM 调用信息
- 记录压缩前后的消息数量和 token 使用情况
- 记录摘要生成失败的警告信息
- 记录消息历史截断的警告信息
- 记录压缩异常的错误信息
- 使用结构化日志格式，包含 agentId、触发原因、压缩结果等信息

### 3.7 压缩历史展示（UI 需求）

**需求 ID**: REQ-3.7

**描述**: 在用户界面中以可展开的形式展示压缩历史，允许用户查看完整的历史消息。

**验收标准**:
- 在对话历史界面中标记压缩点
- 显示压缩摘要内容
- 提供展开/折叠功能，查看被压缩的完整消息
- 显示压缩时间戳
- 显示压缩前后的消息数量统计
- 支持多次压缩的层级展示
- 界面友好，清晰标识压缩区域

## 4. 非功能需求

### 4.1 性能要求

- 摘要生成的 LLM 调用应使用较快的模型（如 gpt-4o-mini）
- 摘要生成超时时间设置为 30 秒
- 自动压缩流程不应显著延长 LLM 调用的响应时间

### 4.2 可靠性要求

- 自动压缩失败不应阻止正常的 LLM 调用
- 摘要生成失败时，忽略本次压缩，等待 tokens 超过硬性限制时截断
- 压缩过程中的错误应被捕获并记录
- 硬性限制截断应确保 tokens 不超限

### 4.3 兼容性要求

- 保持现有 `compress_context` 工具的功能不变
- 智能体仍可主动调用 `compress_context` 工具
- 自动压缩和手动压缩可以共存
- 不影响现有的上下文管理逻辑

## 5. 实现约束

### 5.1 架构约束

- 自动压缩逻辑应集成到 `LlmHandler` 模块中
- 摘要生成功能应作为 `AutoCompressionManager` 的内部方法
- `ConversationManager` 内部使用 `AutoCompressionManager`，不暴露自动压缩相关方法给外部
- 使用现有的 `LlmClient` 进行摘要生成的 LLM 调用
- 不修改 `compress_context` 工具的现有实现

### 5.2 代码约束

- 遵循现有的代码风格和模块划分
- 每个函数添加详细的注释
- 关键算法和流程添加注释说明
- 模块化设计，职责清晰

### 5.3 架构设计原则

**职责划分**：
- **AutoCompressionManager** 是独立的数据处理器，负责所有与压缩相关的逻辑
  - 接收完整的对话数据（包括消息、token 统计、配置等）
  - 自己判断是否需要压缩
  - 自己提取需要压缩的消息
  - 自己生成摘要
  - 自己执行压缩逻辑
  - 返回处理后的数据结构（运行时消息、完整历史、压缩记录）
  
- **ConversationManager** 是数据容器和持久化管理器，职责简化为：
  - 调用 AutoCompressionManager 处理数据
  - 接收处理后的数据并更新内存状态
  - 管理对话数据的持久化
  - 不包含压缩判断逻辑
  - 不包含摘要生成逻辑
  - 不包含压缩执行逻辑

**数据流**：
```
ConversationManager 收集数据
  ↓
传递给 AutoCompressionManager
  ↓
AutoCompressionManager 完整处理（判断、提取、生成、压缩）
  ↓
返回处理结果
  ↓
ConversationManager 更新状态和持久化
```

**API 设计原则**：
- AutoCompressionManager 的 `autoCompress()` 方法接收完整的对话数据对象
- 不使用回调函数或委托模式
- 所有与压缩相关的判断和处理都在 AutoCompressionManager 内部完成
- ConversationManager 只负责数据的收集、传递和结果的应用

### 5.3 测试约束

- 为自动压缩功能编写单元测试
- 测试自动压缩触发条件
- 测试摘要生成功能
- 测试压缩失败的降级处理
- 测试自动压缩和手动压缩的共存

## 6. 用户故事

### 6.1 智能体自动压缩历史

**作为** 智能体系统的用户  
**我希望** 智能体的历史消息能够自动压缩  
**以便** 智能体可以持续工作而不会因为上下文超限而中断

**验收标准**:
- 智能体在处理长时间任务时，历史消息自动压缩
- 智能体无需主动调用 `compress_context` 工具
- 压缩后智能体仍能访问重要的历史信息（通过摘要）
- 压缩过程不影响智能体的正常工作流程

### 6.2 配置自动压缩行为

**作为** 系统管理员  
**我希望** 可以配置自动压缩的触发阈值和保留消息数量  
**以便** 根据不同的使用场景调整压缩策略

**验收标准**:
- 可以通过配置文件设置自动压缩阈值
- 可以设置压缩时保留的最近消息数量
- 可以启用或禁用自动压缩功能
- 配置修改后立即生效

### 6.3 监控自动压缩过程

**作为** 系统管理员  
**我希望** 可以查看自动压缩的日志和统计信息  
**以便** 了解系统的运行状态和优化压缩策略

**验收标准**:
- 日志中记录每次自动压缩的触发时间和原因
- 日志中记录压缩前后的消息数量和 token 使用情况
- 日志中记录摘要生成的耗时和结果
- 日志中记录压缩失败的错误信息

## 7. 实现优先级

### 7.1 高优先级（必须实现）

- REQ-3.1: 自动压缩触发机制
- REQ-3.2: 自动生成压缩摘要
- REQ-3.3: 执行自动压缩（包括完整历史保留）
- REQ-3.4: 硬性限制截断
- REQ-3.5: 配置项管理（包括 summaryModel 配置）
- REQ-3.6: 日志和监控

### 7.2 中优先级（建议实现）

- REQ-3.7: 压缩历史展示（UI 需求）

### 7.3 低优先级（可选实现）

- 自动压缩统计信息的 HTTP API
- 自动压缩历史记录的持久化

## 8. 风险和挑战

### 8.1 摘要质量风险

**风险**: 自动生成的摘要可能丢失重要信息

**缓解措施**:
- 设计高质量的摘要生成提示词
- 保留足够数量的最近消息
- 允许智能体主动调用 `compress_context` 提供自定义摘要
- 摘要生成失败时不执行压缩，等待硬性限制截断

### 8.2 性能影响风险

**风险**: 摘要生成的 LLM 调用可能延长响应时间

**缓解措施**:
- 使用较快的模型生成摘要
- 设置合理的超时时间
- 摘要生成失败时快速降级，不阻塞主流程

### 8.3 上下文超限风险

**风险**: 摘要生成失败后可能导致上下文超限

**缓解措施**:
- 实现硬性限制截断机制
- 截断时保留系统提示词和最近消息
- 记录详细的截断日志
- 持久化截断后的对话历史

## 9. 后续扩展

### 9.1 智能压缩策略

- 根据消息重要性选择性保留
- 根据任务类型调整压缩策略
- 学习用户的压缩偏好

### 9.2 多级压缩

- 支持多次压缩，逐步精简历史
- 保留多个压缩级别的摘要
- 根据需要恢复不同级别的历史

### 9.3 压缩历史持久化

- 将压缩前的完整历史保存到磁盘
- 支持按需恢复完整历史
- 提供历史查询和分析功能
